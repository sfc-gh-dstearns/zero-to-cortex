{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff19af59-933a-4a57-b5fb-d4c836eb63c6",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "# Zero to Cortex  \n",
    "## *Intermediate*\n",
    "This portion of the lab will go through some of the basic principles of creating a Retrieval Augmented Generation application and using LLM's for Data Engineering problems.  \n",
    "https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions\n",
    "\n",
    "![Alt text](https://venturebeat.com/wp-content/uploads/2024/04/a-robot-playing-with-a-snowflake-in-arctic-cinemat-4OYW23nATBm50aD_slLk8w-xaTFE1EbSLmDWJXvWCxXrA.jpeg?fit=750%2C422&strip=all \"a title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.cortex import Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57194229-a642-4a25-9e44-d5ac26819f06",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "## How to build the underpinnings of a Retrieval Augmented Generation (RAG) application  \n",
    "In this section, you will build the infrastructure for a RAG application that helps users find wines by interacting with an AI sommelier. The steps to build the infrastructure are as follows:  \n",
    "1. If your unstructured data requires chunking, do this first. Chunking will split apart the unstructured data into pieces that are smaller (*usually 300-1000 words, but experiment with this!*) that overlap each other to some degree. Each chunk is grouped by its parent data. While this can be done in SQL, Python has great support for doing things like this.\n",
    "    * In most cases, LangChain's [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter) is used.\n",
    "2. Once you have prepared your unstructured data, now you can create embeddings. In Snowflake, it is really easy to create embeddings of your unstructured data. You'll want to create a new column in your table or just create a new table that is used for your RAG application. Here is an example of how you would do this:\n",
    "    * ```  \n",
    "        select\n",
    "            text_field,\n",
    "            text_chunk,\n",
    "            snowflake.cortex.embed_text_768(\n",
    "                'snowflake-arctic-embed-m',\n",
    "                text_chunk\n",
    "            ) as chunk_embedding\n",
    "        from\n",
    "            my_table\n",
    "        ```\n",
    "3. Now you can run similarity queries against your data to get the most relevant chunks of data to feed to your LLM as context. Here is an example of how to do get the top five most relevant pieces of information:  \n",
    "    * ```\n",
    "        select\n",
    "            text_chunk\n",
    "        from\n",
    "            (\n",
    "            select\n",
    "                vector_cosine_similarity(\n",
    "                    chunk_embedding,\n",
    "                    snowflake.cortex.embed_text_768(\n",
    "                        'snowflake-arctic-embed-m',\n",
    "                        '<your text/question will go here>'\n",
    "                    )\n",
    "                ) as similarity\n",
    "            from\n",
    "                my_table\n",
    "            order by\n",
    "                similarity\n",
    "            limit 5\n",
    "            )\n",
    "        ```\n",
    "4. The result from the above query will give you the most relevant chunks of data to provide to your LLM as context. We would combine those chunks of data using Python to feed over to the LLM. Here is how you would combine that result set and provide it to the LLM:  \n",
    "    * ```\n",
    "            # model_name will be the model you select\n",
    "        # This can be Snowflake Arctic or any of the other models we support in Cortex!\n",
    "        model_name = 'snowflake-arctic'\n",
    "        question = \"\"\"<Question from the chat interface>\"\"\"\n",
    "        chunks = session.sql(f\"\"\"\n",
    "            select\n",
    "                text_chunk\n",
    "            from\n",
    "                (\n",
    "                select\n",
    "                    vector_cosine_similarity(\n",
    "                        chunk_embedding,\n",
    "                        snowflake.cortex.embed_text_768(\n",
    "                            'snowflake-arctic-embed-m',\n",
    "                            '{question}'\n",
    "                        )\n",
    "                    ) as similarity\n",
    "                from\n",
    "                    my_table\n",
    "                )\n",
    "            order by\n",
    "                similarity\n",
    "            limit 5\"\"\")\n",
    "        info = '. | '.join([x[0] for x in chunks.select(\"*\").collect()]).replace(\"'\", \"\")\n",
    "        prompt = f\"\"\"\n",
    "                    <YOUR PROMPT>\n",
    "                    Answer the questions based on the context provided between the <context> and </context> tags. The\n",
    "                    question will be found between the <question> and </question> tags.\n",
    "                    <context>\n",
    "                    '{info}'\n",
    "                    </context>\n",
    "                    <question>\n",
    "                    '{info}'\n",
    "                    </question>\n",
    "                    Answer: \"\"\"\n",
    "        query = \"\"\"\n",
    "                select\n",
    "                    snowflake.cortex.complete(\n",
    "                    # model name goes here\n",
    "                        ?, \n",
    "                    # prompt goes here\n",
    "                        ?\n",
    "                    ) as response\n",
    "                \"\"\"\n",
    "        complete = session.sql(query, params=[model_name, prompt])\n",
    "        complete.collect()[0][0]\n",
    "        ```\n",
    "**That's it!** You have successfully created the underpinnings of a RAG application. Your LLM will have pointed context to your questions. You can optionally add some more complexity here with LLM hyperparameters or by also providing the chat history for further context. Additionally, when we are dealing with millions of rows of data, we should opt to use the Snowflake Cortex Search API. This is a brand new offering (currently in Private Preview as of 5/3/24).  \n",
    "``` Cortex Search enables low-latency, high-quality search over your Snowflake data.\n",
    "Cortex Search powers a broad array of search experiences for Snowflake users including \n",
    "Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n",
    "\n",
    "Cortex Search gets you up and running with a vector and keyword-based search engine on your\n",
    "text data in minutes, without having to worry about embedding, infrastructure maintenance, \n",
    "search quality parameter tuning, or ongoing index refreshes. This means you can spend less \n",
    "time on infrastructure and search quality tuning, and more time developing high-quality \n",
    "chat and search experiences using your data.\n",
    "```  \n",
    "## Let's run some sample queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f1fc4-7d24-475d-ab9e-9434609dcaa4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "model_name = st.radio(\n",
    "    label = \"Choose your model\",\n",
    "    options = [\n",
    "        \"snowflake-arctic\",\n",
    "        \"mistral-large\",\n",
    "        \"mixtral-8x7b\",\n",
    "        \"mistral-7b\",\n",
    "        \"reka-flash\",\n",
    "        \"reka-core\",\n",
    "        \"llama2-70b-chat\",\n",
    "        \"llama3-70b\",\n",
    "        \"llama3-8b\",\n",
    "        \"gemma-7b\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac18a8-00c4-4694-83fa-d43c5e3b567a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Enter your question here!\n",
    "question = \"\"\"I need a list of the top three white wines to pair well with seafood. \n",
    "                Please provide the wine, some information about the wine, and the price. Also, add up the \n",
    "                total cost of the list.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13052a-034f-456d-a5da-88f783721bfa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# Get the relevant data\n",
    "chunks = session.sql(f\"\"\"\n",
    "  select\n",
    "      full_description\n",
    "  from\n",
    "      (\n",
    "        select\n",
    "            full_description,\n",
    "            vector_cosine_similarity(\n",
    "                information_embeds,\n",
    "                snowflake.cortex.embed_text_768(\n",
    "                  'snowflake-arctic-embed-m',\n",
    "                  '{question}'\n",
    "                )\n",
    "            ) as similarity\n",
    "        from\n",
    "            wine_reviews\n",
    "        )\n",
    "  order by\n",
    "      similarity\n",
    "  limit 10\"\"\")\n",
    "chunks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1774e2b-f890-45f0-8542-a60b4a1a8095",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Give the context to the LLM and get your question answered!\n",
    "info = '. | '.join([x[0] for x in chunks.select(\"*\").collect()]).replace(\"'\", \"\")\n",
    "prompt = f\"\"\"\n",
    "            You are a world class sommelier. Our guests seek knowledge and guidance. Do not hallucinate, the guests don't like that.\n",
    "            If you don't know what to recommend just say so. Please answer the questions based on the provided \n",
    "            context found between the tags <context> and </context>. The guest's question will be between the\n",
    "            <question> and </question> tags. Please present the wine nicely. Explain where it is from, the variety of wine it is, and the price.\n",
    "            Answer the questions based on the context provided between the <context> and </context> tags. The\n",
    "            question will be found between the <question> and </question> tags.\n",
    "            <context>\n",
    "            '{info}'\n",
    "            </context>\n",
    "            <question>\n",
    "            '{question}'\n",
    "            </question>\n",
    "            Answer: \"\"\"\n",
    "query = \"\"\"\n",
    "      select\n",
    "          snowflake.cortex.complete(\n",
    "              ?, \n",
    "              ?\n",
    "          ) as response\n",
    "      \"\"\"\n",
    "complete = session.sql(query, params=[model_name, prompt])\n",
    "with st.chat_message(name=\"Assistant\"):\n",
    "    st.write(complete.collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c05bc3-f547-4333-834e-4b3b92963bc9",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## Another interesting use case - *data cleaning*  \n",
    "In this next example, you will see that one would have to write some Regex to extract information from the `variable` column. What if we could do this with LLMs instead of writing complex code? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb054126-9a8d-4e2f-a8d2-cb15f0112a29",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "select * from sec_filings limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fccd3e-e737-4c19-8b5d-e9e259a29b4a",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "-- Create a new column called CLASSIFICATION from the VARIABLE column\n",
    "-- This column will give you a clean document type classification\n",
    "select\n",
    "    variable,\n",
    "    snowflake.cortex.complete(\n",
    "        'snowflake-arctic',\n",
    "        CONCAT('Based on the value between the <variable> and </variable> tags, please classify\n",
    "            the data in ONLY one of these three categories: 10K, 10Q, 8K. If you cannot classify\n",
    "            the data based on the information, impute NULL. Do not provide an explanation. Only provide \n",
    "            your answer of 10K, 10Q, 8K, or NULL.\n",
    "            <variable>', variable, '</variable>'\n",
    "        )) as classification\n",
    "from\n",
    "    sec_filings\n",
    "limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d0b8f-6770-49ed-8293-dd5d286e1728",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
