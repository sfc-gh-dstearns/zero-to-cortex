{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff19af59-933a-4a57-b5fb-d4c836eb63c6",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "# Zero to Cortex  \n",
    "## *Intermediate*\n",
    "This portion of the lab will go through some of the basic principles of creating a Retrieval Augmented Generation application and using LLM's for Data Engineering problems.  \n",
    "https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions\n",
    "\n",
    "![Alt text](https://venturebeat.com/wp-content/uploads/2024/04/a-robot-playing-with-a-snowflake-in-arctic-cinemat-4OYW23nATBm50aD_slLk8w-xaTFE1EbSLmDWJXvWCxXrA.jpeg?fit=750%2C422&strip=all \"a title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport json\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nfrom snowflake.snowpark import functions as F\nfrom snowflake.cortex import Complete\nfrom snowflake.core import Root\nroot = Root(session)"
  },
  {
   "cell_type": "markdown",
   "id": "57194229-a642-4a25-9e44-d5ac26819f06",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": "## How to build the underpinnings of a Retrieval Augmented Generation (RAG) application  \nIn this section, you will build the infrastructure for a RAG application that helps users find wines by interacting with an AI sommelier. The steps to build the infrastructure are as follows:  \n1. If your unstructured data requires chunking, do this first. Chunking will split apart the unstructured data into pieces that are smaller (*usually 300-1000 words, but experiment with this!*) that overlap each other to some degree. Each chunk is grouped by its parent data. While this can be done in SQL, Python has great support for doing things like this.\n    * In most cases, LangChain's [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter) is used.\n2. Once you have prepared your unstructured data, now you can create embeddings. In Snowflake, it is really easy to create embeddings of your unstructured data. You'll want to create a new column in your table or just create a new table that is used for your RAG application. Here is an example of how you would do this:\n    * ```  \n        select\n            text_field,\n            text_chunk,\n            snowflake.cortex.embed_text_768(\n                'snowflake-arctic-embed-m',\n                text_chunk\n            ) as chunk_embedding\n        from\n            my_table\n        ```\n3. Now you can run similarity queries against your data to get the most relevant chunks of data to feed to your LLM as context. Here is an example of how to do get the top five most relevant pieces of information:  \n    * ```\n        select\n            text_chunk\n        from\n            (\n            select\n                vector_cosine_similarity(\n                    chunk_embedding,\n                    snowflake.cortex.embed_text_768(\n                        'snowflake-arctic-embed-m',\n                        '<your text/question will go here>'\n                    )\n                ) as similarity\n            from\n                my_table\n            order by\n                similarity desc\n            limit 5\n            )\n        ```\n4. The result from the above query will give you the most relevant chunks of data to provide to your LLM as context. We would combine those chunks of data using Python to feed over to the LLM. Here is how you would combine that result set and provide it to the LLM:  \n    * ```\n            # model_name will be the model you select\n        # This can be Snowflake Arctic or any of the other models we support in Cortex!\n        model_name = 'snowflake-arctic'\n        question = \"\"\"<Question from the chat interface>\"\"\"\n        chunks = session.sql(f\"\"\"\n            select\n                text_chunk\n            from\n                (\n                select\n                    vector_cosine_similarity(\n                        chunk_embedding,\n                        snowflake.cortex.embed_text_768(\n                            'snowflake-arctic-embed-m',\n                            '{question}'\n                        )\n                    ) as similarity\n                from\n                    my_table\n                )\n            order by\n                similarity desc\n            limit 5\"\"\")\n        info = '. | '.join([x[0] for x in chunks.select(\"*\").collect()]).replace(\"'\", \"\")\n        prompt = f\"\"\"\n                    <YOUR PROMPT>\n                    Answer the questions based on the context provided between the <context> and </context> tags. The\n                    question will be found between the <question> and </question> tags.\n                    <context>\n                    '{info}'\n                    </context>\n                    <question>\n                    '{info}'\n                    </question>\n                    Answer: \"\"\"\n        query = \"\"\"\n                select\n                    snowflake.cortex.complete(\n                    # model name goes here\n                        ?, \n                    # prompt goes here\n                        ?\n                    ) as response\n                \"\"\"\n        complete = session.sql(query, params=[model_name, prompt])\n        complete.collect()[0][0]\n        ```\n**That's it!** You have successfully created the underpinnings of a RAG application. Your LLM will have pointed context to your questions. You can optionally add some more complexity here with LLM hyperparameters or by also providing the chat history for further context. Additionally, when we are dealing with millions of rows of data, we should opt to use the Snowflake Cortex Search API. This is a brand new offering. **Now, you don't need to handle embeddings and vector calculations anymore, but it is good practice to understand them.** \n``` Cortex Search enables low-latency, high-quality search over your Snowflake data.\nCortex Search powers a broad array of search experiences for Snowflake users including \nRetrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n\nCortex Search gets you up and running with a vector and keyword-based search engine on your\ntext data in minutes, without having to worry about embedding, infrastructure maintenance, \nsearch quality parameter tuning, or ongoing index refreshes. This means you can spend less \ntime on infrastructure and search quality tuning, and more time developing high-quality \nchat and search experiences using your data.\n```  \n## Let's run some sample queries!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac18a8-00c4-4694-83fa-d43c5e3b567a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "# Enter your question here!\nquestion = \"\"\"I like red wines from California\"\"\""
  },
  {
   "cell_type": "code",
   "id": "d7e6b1b3-dcb1-403d-b84e-302bbe5968af",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "model_name = st.radio(\n    label = \"Choose your model\",\n    options = [\n        \"snowflake-arctic\",\n        \"mistral-large\",\n        \"mixtral-8x7b\",\n        \"mistral-7b\",\n        \"reka-flash\",\n        \"llama2-70b-chat\",\n        \"llama3-70b\",\n        \"llama3-8b\",\n        \"gemma-7b\"\n    ]\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d582c65c-f034-476f-9a09-a87017530a4a",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "### This is a way to find similar data via manual embedding and vector search. Useful, but does require some set up. You do need to embed your data and save the vectors with your data ahead of time. Additionally, you have to embed your query on the fly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13052a-034f-456d-a5da-88f783721bfa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Get the relevant data\nchunks = session.sql(f\"\"\"\n  select\n      full_description\n  from\n      (\n        select\n            full_description,\n            vector_cosine_similarity(\n                information_embeds,\n                snowflake.cortex.embed_text_768(\n                  'snowflake-arctic-embed-m',\n                  '{question}'\n                )\n            ) as similarity\n        from\n            wine_reviews\n        )\n  order by\n      similarity desc\n  limit 10\"\"\")\nchunks.show()"
  },
  {
   "cell_type": "markdown",
   "id": "a3d82797-3521-4972-b38e-4382e3ea75ac",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "### Here is another, much faster way to do this context search. *Cortex Search* performs low latency vector search without any of the need to create those embeddings ahead of time, write the code to embed your query, or perform the vector search. Cortex Search performs all of this for you on top of an indexed \"vector DB\"-like table. High performance on large scales."
  },
  {
   "cell_type": "code",
   "id": "61d026e6-5d5e-4fa8-aeac-911359f21403",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Instantiate the pointer to the Search Service\nwine_ss = (root\n  .databases[\"z2c\"]\n  .schemas[\"cortex\"]\n  .cortex_search_services[\"wine_search_service\"]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15418df9-3b18-46f8-91f9-2b37dd562abf",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "resp = wine_ss.search(\n  query=question,\n  columns=[\"full_description\"],\n  limit=10\n)\nread_out = [x[\"full_description\"] for x in json.loads(resp.to_json())[\"results\"]]\ninfo = '. | '.join([x[\"full_description\"] for x in json.loads(resp.to_json())[\"results\"]]).replace(\"'\", \"\")\nread_out",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1774e2b-f890-45f0-8542-a60b4a1a8095",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Give the context to the LLM and get your question answered!\nprompt = f\"\"\"\n                You are an airline specialist. You have access to a plethora of reviews about airlines for thousands of different routes.\n                Only utilize the context provided between the tags <context> and </context>. The user's question will be between the\n                <question> and </question> tags. Please present the wine nicely. Explain where it is from, the variety of wine it is, and the price.\n            Answer the questions based on the context provided between the <context> and </context> tags. The\n            question will be found between the <question> and </question> tags.\n            <context>\n            '{info}'\n            </context>\n            <question>\n            '{question}'\n            </question>\n            Answer: \"\"\"\nquery = \"\"\"\n      select\n          snowflake.cortex.complete(\n              ?, \n              ?\n          ) as response\n      \"\"\"\ncomplete = session.sql(query, params=[model_name, prompt])\nwith st.chat_message(name=\"Assistant\"):\n    st.write(complete.collect()[0][0])"
  },
  {
   "cell_type": "markdown",
   "id": "d0c05bc3-f547-4333-834e-4b3b92963bc9",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## Another interesting use case - *data cleaning*  \n",
    "In this next example, you will see that one would have to write some Regex to extract information from the `variable` column. What if we could do this with LLMs instead of writing complex code? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb054126-9a8d-4e2f-a8d2-cb15f0112a29",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "select * from sec_filings limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fccd3e-e737-4c19-8b5d-e9e259a29b4a",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": "-- Create a new column called CLASSIFICATION from the VARIABLE column\n-- This column will give you a clean document type classification\nselect\n    sec_document_id,\n    snowflake.cortex.complete(\n        'snowflake-arctic',\n        CONCAT('Based on the value between the <variable> and </variable> tags, please classify\n            the data in ONLY one of these three categories: 10K, 10Q, 8K. If you cannot classify\n            the data based on the information, impute NULL. Do not provide an explanation. Only provide \n            your answer of 10K, 10Q, 8K, or NULL.\n            <variable>', sec_document_id, '</variable>'\n        )) as classification\nfrom\n    sec_filings\nlimit 20"
  },
  {
   "cell_type": "markdown",
   "id": "265a9020-7add-4c5b-8397-7b7892b5d880",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": [
    "### De-Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d0b8f-6770-49ed-8293-dd5d286e1728",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "create or replace table emails (\n",
    "email string\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fad91-8ce6-40e1-b794-086f21217c9d",
   "metadata": {
    "name": "cell13",
    "language": "sql",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "insert into emails\n",
    "values\n",
    "('hello world. david.stearns@snowflake.com hello world david.stearns@snowflake.com'),\n",
    "('hello world. robert.silva@snowflake.com hello world robert.silva@snowflake.com'),\n",
    "('hello world. frank.slootman@retired.com hello world frank.slootman@retired.com'),\n",
    "('hi internal people. scott.lief@gr.com hi internal folks scott.lief@gr.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2359a1a-493a-4cd6-846c-620e8d00a922",
   "metadata": {
    "name": "cell14",
    "language": "sql",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select\n    email,\n    snowflake.cortex.complete(\n        'snowflake-arctic',\n        CONCAT(\n            'Given the following sentences supplied between the <email> and </email> tags, please perform the following                        transformation:\n            For all email addresses within the string, replace the identifiable portion with xxxx. For example, hi there.                   david.stearns@snowflake.com hi there david.stearns@snowflake.com should become hi, there. xxxx@snowflake.com hi                 there xxxx@snowflake.com ... Only respond with the transformed string.\n            <email>',\n            email,\n            '</email>'\n            )\n    ) as clean_string,\n    snowflake.cortex.complete(\n        'snowflake-arctic',\n        CONCAT(\n            'Based on the data between <email> and </email>, what email domains are in the data? If there are multiple, list                them. If all domains in the data are the same, just give the domain once. Only respond with the identified domain,              DO NOT PROVIDE AN EXPLANATION.\n            <email>', \n            email, \n            '</email>'\n                )\n    ) as domains,\n    snowflake.cortex.complete(\n        'snowflake-arctic',\n        CONCAT(\n            'Based on the data between <email> and </email>, what are the names of the people listed in the email address? Only             respond with the name of the person, nothing else. First and last names are sometimes separated by a period. Remove the period and add a space if necessary.\n            <email>',\n            email, \n            '</email>'\n                )\n    ) as names\nfrom\n    emails;"
  },
  {
   "cell_type": "code",
   "id": "2b9c1719-fbf6-4d00-a995-468a34a64adb",
   "metadata": {
    "language": "sql",
    "name": "cell15",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}